{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for Bedrock\ntemperature\n  extra fields not permitted (type=value_error.extra)\ntop_k\n  extra fields not permitted (type=value_error.extra)\ntop_p\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bedrock\n\u001b[1;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mBedrock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials_profile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamazon.titan-text-express-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m conversation \u001b[38;5;241m=\u001b[39m ConversationChain(\n\u001b[0;32m     15\u001b[0m   llm\u001b[38;5;241m=\u001b[39mllm, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, memory\u001b[38;5;241m=\u001b[39mConversationBufferMemory()\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m conversation\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi Bedrock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\codes\\yknlp\\lib\\site-packages\\langchain_core\\load\\serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32md:\\codes\\yknlp\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 3 validation errors for Bedrock\ntemperature\n  extra fields not permitted (type=value_error.extra)\ntop_k\n  extra fields not permitted (type=value_error.extra)\ntop_p\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    credentials_profile_name=\"default\", \n",
    "    model_id=\"amazon.titan-text-express-v1\",\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9,\n",
    "    top_k = 50\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "  llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi Bedrock\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi, how was going?\n",
      "AI:\u001b[0m\n",
      " Hello! I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't actually have experiences to recount. How has your day been going so far?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello! I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't actually have experiences to recount. How has your day been going so far?\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    credentials_profile_name=\"default\",\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"hi, how was going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggest the name of a company that makes home robots\n",
      " Here are some suggestions for company names that make home robots:\n",
      "\n",
      "- Anki - Makes the Vector and Cozmo home robots.\n",
      "\n",
      "- Mayfield Robotics - Created Kuri, a cute home robot. Unfortunately they went out of business. \n",
      "\n",
      "- Ubtech - Makes the Lynx home robot that can recognize faces and have conversations.\n",
      "\n",
      "- Aeolus Robotics - Makes home robots focused on cleaning like vacuuming.\n",
      "\n",
      "- Savioke - Makes delivery robots for hotels and hospitals.\n",
      "\n",
      "- Misty Robotics - Created Misty, a customizable home robot using robotics and AI. \n",
      "\n",
      "- Temi - Makes a personal assistant robot that can follow you around home. \n",
      "\n",
      "- Toyota - Developed the Human Support Robot to assist people at home.\n",
      "\n",
      "- SoftBank/Aldebaran - Makes the Pepper social/home robot.\n",
      "\n",
      "- Pal Robotics - Makes the Tiago and Reem home robots. \n",
      "\n",
      "- LG - Released the Hub Robot for smart home control.\n",
      "\n",
      "So in summary, some of the top companies in home/social robots are Anki, Ubtech, Misty, Temi, Toyota, SoftBank, Pal and LG."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Here are some suggestions for company names that make home robots:\\n\\n- Anki - Makes the Vector and Cozmo home robots.\\n\\n- Mayfield Robotics - Created Kuri, a cute home robot. Unfortunately they went out of business. \\n\\n- Ubtech - Makes the Lynx home robot that can recognize faces and have conversations.\\n\\n- Aeolus Robotics - Makes home robots focused on cleaning like vacuuming.\\n\\n- Savioke - Makes delivery robots for hotels and hospitals.\\n\\n- Misty Robotics - Created Misty, a customizable home robot using robotics and AI. \\n\\n- Temi - Makes a personal assistant robot that can follow you around home. \\n\\n- Toyota - Developed the Human Support Robot to assist people at home.\\n\\n- SoftBank/Aldebaran - Makes the Pepper social/home robot.\\n\\n- Pal Robotics - Makes the Tiago and Reem home robots. \\n\\n- LG - Released the Hub Robot for smart home control.\\n\\nSo in summary, some of the top companies in home/social robots are Anki, Ubtech, Misty, Temi, Toyota, SoftBank, Pal and LG.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    credentials_profile_name=\"default\",\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"product\"],\n",
    "  template=\"suggest the name of a company that makes {product}\"\n",
    ")\n",
    "\n",
    "print(prompt.format(product=\"home robots\"))\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm,\n",
    "  prompt=prompt\n",
    ")\n",
    "\n",
    "chain.run(\"home robots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"b5cda8e1cd13ddba2efff470bf43f686053ba2e2\"\n",
    "\n",
    "bedrock = Bedrock(\n",
    "    credentials_profile_name=\"default\", \n",
    "    model_id=\"anthropic.claude-v2\"\n",
    ")\n",
    "\n",
    "tools = load_tools(\n",
    "  tool_names=[\"serpapi\",\"llm-math\"],\n",
    "  llm = bedrock #already defined on the upper block\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "  agent=\"zero-shot-react-description\",\n",
    "  llm=bedrock,\n",
    "  tools=tools,\n",
    "  verbose=True,\n",
    "  handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# agent.run(\"calculate 123*4 with calculator\")\n",
    "agent.run(\"I need the weather in Seoul today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Who is Leo DiCaprio's girlfriend? No need to add Action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" That's a cute name for a pet! I don't actually have any personal pets or know specifics about your pet Coco, since I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. But I'm happy to chat more about pets and pet names if you'd like!\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "chain = ConversationChain(llm=bedrock)\n",
    "\n",
    "chain.run(\"my pet's name is coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\codes\\yknlp\\lib\\site-packages\\langchain_community\\llms\\bedrock.py:52: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: my pet's name is coco\n",
      "AI:  That's a cute name for a pet! I don't actually have any personal pets or know specifics about your pet Coco, since I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. But I'm happy to chat more about pets and pet names if you'd like!\n",
      "\n",
      "Human: what is my pet's name?\n",
      "AI:\n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm afraid I don't know your pet's name. You had mentioned your pet is named Coco, but since I'm an AI without personal experiences, I don't actually know any specifics about your own pet or its name unless you provide that information. I'd be happy to continue our conversation though!\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"what is my pet's name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 고양이의 주요 울음소리는 다음과 같습니다:\n",
      "\n",
      "- 명랑한 울음소리 - \"냐옹\" 또는 \"야옹\"과 같이 높은 음으로 울 때가 많습니다. 긍정적인 감정을 표현할 때 나타납니다.\n",
      "\n",
      "- 불쾌한 울음소리 - \"내우\" 또는 \"야우\"와 같이 낮고 불만스러운 소리를 냅니다. 불쾌함이나 공격성을 표현할 때 나타납니다. \n",
      "\n",
      "- 고통스러운 울음소리 - \"미야우\" 또는 \"미외\"와 같이 높고 갸냘픈 소리를 냅니다. 아프거나 놀랐을 때 나타납니다.\n",
      "\n",
      "- 구걸하는 울음소"
     ]
    }
   ],
   "source": [
    "result = llm('고양이의 울음소리는?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 고양이의 주요 울음소리는 다음과 같습니다:\n",
      "\n",
      "- 명랑한 울음소리 - \"냐옹\" 또는 \"야옹\"과 같이 높은 음으로 울 때가 많습니다. 긍정적인 감정을 표현할 때 나타납니다.\n",
      "\n",
      "- 불쾌한 울음소리 - \"내우\" 또는 \"야우\"와 같이 낮고 불만스러운 소리를 냅니다. 불쾌함이나 공격성을 표현할 때 나타납니다. \n",
      "\n",
      "- 고통스러운 울음소리 - \"미야우\" 또는 \"미외\"와 같이 높고 갸냘픈 소리를 냅니다. 아프거나 놀랐을 때 나타납니다.\n",
      "\n",
      "- 구걸하는 울음소\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 고양이의 울음소리는 다음과 같습니다:\n",
      "\n",
      "- 야옹 야옹 - 고양이가 먹이를 달라고 울 때 주로 나는 소리\n",
      "\n",
      "- 냐옹 냐옹 - 고양이가 불편함이나 고통을 느낄 때 내는 울음소리\n",
      "\n",
      "- 미야옹 - 고양이가 교미를 원할 때 울리는 소리\n",
      "\n",
      "- 으르렁 거리는 소리 - 고양이가 위협을 느낄 때 내는 낮은 울음소리\n",
      "\n",
      "- 웅크리는 소리 - 고양이가 무서워할 때 나는 높은 울음소리\n",
      "\n",
      "- 꾸르륵 거리는 소리 - 고양이가 만족감을  까마귀의 울음소리는 다음과 같습니다:\n",
      "\n",
      "- 크앙크앙 혹은 캬캬캬 \n",
      "- 개개개거리는 소리\n",
      "- 깡총깡총 하는 불쾌한 울음소리\n",
      "\n",
      "까마귀는 일반적으로 '크앙크앙' 혹은 '캬캬캬'와 같은 크고 거친 울음소리를 내며, 때때로 개개거리는 듯한 소리를 내기도 합니다. 또한 불쾌하고 거슬리는 '깡총깡총' 소리를 울 때도 있습니다. 까마귀의 울음소리는 대체로 낮은 음고이면서 거칠고 불쾌한 느낌을 주"
     ]
    }
   ],
   "source": [
    "result = llm.generate([\"고양이의 울음소리는?\", \"까마귀의 울음소리는?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response0:  고양이의 울음소리는 다음과 같습니다:\n",
      "\n",
      "- 야옹 야옹 - 고양이가 먹이를 달라고 울 때 주로 나는 소리\n",
      "\n",
      "- 냐옹 냐옹 - 고양이가 불편함이나 고통을 느낄 때 내는 울음소리\n",
      "\n",
      "- 미야옹 - 고양이가 교미를 원할 때 울리는 소리\n",
      "\n",
      "- 으르렁 거리는 소리 - 고양이가 위협을 느낄 때 내는 낮은 울음소리\n",
      "\n",
      "- 웅크리는 소리 - 고양이가 무서워할 때 나는 높은 울음소리\n",
      "\n",
      "- 꾸르륵 거리는 소리 - 고양이가 만족감을 \n",
      "--------------------------------------------------\n",
      "response1:  까마귀의 울음소리는 다음과 같습니다:\n",
      "\n",
      "- 크앙크앙 혹은 캬캬캬 \n",
      "- 개개개거리는 소리\n",
      "- 깡총깡총 하는 불쾌한 울음소리\n",
      "\n",
      "까마귀는 일반적으로 '크앙크앙' 혹은 '캬캬캬'와 같은 크고 거친 울음소리를 내며, 때때로 개개거리는 듯한 소리를 내기도 합니다. 또한 불쾌하고 거슬리는 '깡총깡총' 소리를 울 때도 있습니다. 까마귀의 울음소리는 대체로 낮은 음고이면서 거칠고 불쾌한 느낌을 주\n",
      "llm_output: None\n"
     ]
    }
   ],
   "source": [
    "print(\"response0:\", result.generations[0][0].text)\n",
    "print(\"-\"*50)\n",
    "print(\"response1:\", result.generations[1][0].text)\n",
    "\n",
    "print(\"llm_output:\", result.llm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m   SystemMessage,\n\u001b[0;32m      3\u001b[0m   HumanMessage,\n\u001b[0;32m      4\u001b[0m   AIMessage\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatAnthropic\n\u001b[1;32m----> 9\u001b[0m chat_llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatAnthropic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manthropic.claude-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m   HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m고양이의 울음소리는?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m chat_llm(messages)\n",
      "File \u001b[1;32md:\\codes\\yknlp\\lib\\site-packages\\langchain_core\\load\\serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32md:\\codes\\yknlp\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "  SystemMessage,\n",
    "  HumanMessage,\n",
    "  AIMessage\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "chat_llm = ChatAnthropic(\n",
    "  model_name=\"anthropic.claude-v2\",\n",
    "  temperature=0.9\n",
    ")\n",
    "\n",
    "messages = [\n",
    "  HumanMessage(content=\"고양이의 울음소리는?\")\n",
    "]\n",
    "\n",
    "result = chat_llm(messages)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 하늘의 색깔은 주로 파란색이나 푸른색을 띕니다.\n",
      "\n",
      "하늘의 색깔은 대기 중의 여러 가지 요인에 의해 결정됩니다:\n",
      "\n",
      "- 일차적으로는 대기 중의 가스와 먼지 입자들이 햇빛을 산란시키기 때문입니다. 햇빛은 모든 파장의 빛을 포함하고 있지만, 대기 중의 입자들이 짧은 파장의 빛(보라색, 파랑색)을 더 강하게 산란시키기 때문에 하늘은 보라색과 파랑색이 가장 증가한 혼합색인 푸른색을 띠게 됩니다. \n",
      "\n",
      "- 두 번째로는 대기 중의 수증기 함량도 영향을 줍니다. 수증기"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' 하늘의 색깔은 주로 파란색이나 푸른색을 띕니다.\\n\\n하늘의 색깔은 대기 중의 여러 가지 요인에 의해 결정됩니다:\\n\\n- 일차적으로는 대기 중의 가스와 먼지 입자들이 햇빛을 산란시키기 때문입니다. 햇빛은 모든 파장의 빛을 포함하고 있지만, 대기 중의 입자들이 짧은 파장의 빛(보라색, 파랑색)을 더 강하게 산란시키기 때문에 하늘은 보라색과 파랑색이 가장 증가한 혼합색인 푸른색을 띠게 됩니다. \\n\\n- 두 번째로는 대기 중의 수증기 함량도 영향을 줍니다. 수증기')]], llm_output=None, run=[RunInfo(run_id=UUID('d844ebcc-479d-432d-be35-3bbb7ea08aec'))])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "llm.generate([\"하늘의 색깔은?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' 하늘의 색깔은 주로 파란색이나 푸른색을 띕니다.\\n\\n하늘의 색깔은 대기 중의 여러 가지 요인에 의해 결정됩니다:\\n\\n- 일차적으로는 대기 중의 가스와 먼지 입자들이 햇빛을 산란시키기 때문입니다. 햇빛은 모든 파장의 빛을 포함하고 있지만, 대기 중의 입자들이 짧은 파장의 빛(보라색, 파랑색)을 더 강하게 산란시키기 때문에 하늘은 보라색과 파랑색이 가장 증가한 혼합색인 푸른색을 띠게 됩니다. \\n\\n- 두 번째로는 대기 중의 수증기 함량도 영향을 줍니다. 수증기')]], llm_output={}, run=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate([\"하늘의 색깔은?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQ: How can I play guitar well?\n",
      "A: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Here are some tips for learning to play guitar well:\\n\\n- Take lessons. Working with an experienced guitar teacher is the best way to learn proper technique and build a strong foundation. They can guide you through the basics and help you progress.\\n\\n- Practice regularly. Set aside time to practice every day, even if just for 15-30 minutes. Consistency is key when building your skills. Practice scales, chords, songs, exercises and more.\\n\\n- Master the basics. Learn proper hand positioning, fretting, strumming and picking techniques. Start with simple chords and easy songs and gradually increase the difficulty as you improve.\\n\\n- Use the correct posture and hold the guitar properly. Sit up straight, keep your wrists straight and bring the guitar close to your body. This will make playing easier.\\n\\n- Build finger strength and dexterity. Your fingers will hurt at first but keep playing and they will develop calluses and get stronger. Do stretches and exercises too.\\n\\n- Learn to read tablature and music notation. These are the languages of guitar music and essential reading skills. \\n\\n- Learn songs you enjoy. Choose songs you like and that fit your skill level. Practice the chords and solos until you can'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain #Generic chain \n",
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#Create Template \n",
    "template = \"\"\"Q: {question}\n",
    "A: \"\"\"\n",
    "\n",
    "#Create Prompt template \n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"question\"],\n",
    "  template=template\n",
    ")\n",
    "\n",
    "#Create LLMChain \n",
    "\n",
    "llm_chain = LLMChain(\n",
    "  llm=Bedrock(    \n",
    "    credentials_profile_name=\"default\", \n",
    "    model_id=\"anthropic.claude-v2\"),\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "question = \"How can I play guitar well?\"\n",
    "llm_chain.predict(question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m write a poem with Cat topic \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Here is a cat poem I wrote for you:\n",
      "\n",
      "Ode to a Cat\n",
      "\n",
      "With fur so soft and eyes so bright, \n",
      "My cat is quite a lovely sight.\n",
      "Perched upon the windowsill, \n",
      "Watching birds that she may kill.\n",
      "When she's not engaged in hunting,\n",
      "Through the house she goes a-romping.\n",
      "Underfoot wherever I may go,\n",
      "Tripping me with tail in tow.\n",
      "At night upon my bed she'll creep,\n",
      "Curling up with me to sleep.\n",
      "Though with claws she gives a scrape,\n",
      "Holding her I can't escape.\n",
      "She's a mystery I can't decode,\n",
      "This feline that I call my own.\n",
      "I wouldn't trade her for a mouse,\n",
      "My precious purring kitty house.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\" write a {target} with {subject} topic \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template,\n",
    "  input_variables=[\"target\", \"subject\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "  llm=Bedrock(    \n",
    "    credentials_profile_name=\"default\", \n",
    "    model_id=\"anthropic.claude-v2\"),\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "print(llm_chain.predict(subject=\"Cat\", target=\"poem\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yknlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
